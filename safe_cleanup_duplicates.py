#!/usr/bin/env python3

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from database_handler import SupabaseHandler
from datetime import datetime, timedelta
from collections import defaultdict
import json

def create_safe_cleanup():
    """–°–æ–∑–¥–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–ª–∞–Ω –æ—á–∏—Å—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
    
    print("üîß –°–û–ó–î–ê–ù–ò–ï –ë–ï–ó–û–ü–ê–°–ù–û–ì–û –ü–õ–ê–ù–ê –û–ß–ò–°–¢–ö–ò")
    print("====================================")
    
    supabase_handler = SupabaseHandler()
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–æ–∑–∏—Ü–∏–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π (—á—Ç–æ–±—ã –ø–æ–π–º–∞—Ç—å –≤—Å–µ –¥—É–±–ª–∏–∫–∞—Ç—ã)
    week_ago = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
    
    print(f"üìÖ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–ø–∏—Å–∏ —Å {week_ago}...")
    
    result = supabase_handler.client.table('lp_position_snapshots').select(
        'id, position_mint, network, pool_id, pool_name, position_value_usd, created_at'
    ).gte('created_at', week_ago).order('created_at', desc=True).execute()
    
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(result.data)} –∑–∞–ø–∏—Å–µ–π")
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –ø–æ–∑–∏—Ü–∏—è–º
    unique_positions = defaultdict(list)
    
    for record in result.data:
        position_mint = record.get('position_mint', '')
        network = record.get('network', '')
        value = float(record.get('position_value_usd', 0))
        created_at = record.get('created_at', '')
        
        # –ö–ª—é—á —É–Ω–∏–∫–∞–ª—å–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏
        unique_key = f"{position_mint}_{network}"
        
        unique_positions[unique_key].append({
            'id': record['id'],
            'pool_name': record.get('pool_name', 'N/A'),
            'value': value,
            'created_at': created_at
        })
    
    # –°–æ–∑–¥–∞–µ–º –ø–ª–∞–Ω –æ—á–∏—Å—Ç–∫–∏
    cleanup_plan = {
        'total_positions': len(unique_positions),
        'duplicates_found': 0,
        'records_to_delete': [],
        'records_to_keep': [],
        'total_excess_value': 0,
        'created_at': datetime.now().isoformat()
    }
    
    print(f"\nüîç –ê–ù–ê–õ–ò–ó {len(unique_positions)} –£–ù–ò–ö–ê–õ–¨–ù–´–• –ü–û–ó–ò–¶–ò–ô:")
    
    for unique_key, records in unique_positions.items():
        if len(records) > 1:
            cleanup_plan['duplicates_found'] += 1
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ —Å–æ–∑–¥–∞–Ω–∏—è (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
            records.sort(key=lambda x: x['created_at'], reverse=True)
            
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Å–∞–º—É—é —Å–≤–µ–∂—É—é –∑–∞–ø–∏—Å—å
            newest = records[0]
            duplicates = records[1:]
            
            cleanup_plan['records_to_keep'].append({
                'id': newest['id'],
                'pool_name': newest['pool_name'],
                'value': newest['value'],
                'created_at': newest['created_at'],
                'unique_key': unique_key
            })
            
            # –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–º–µ—á–∞–µ–º –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ
            for dup in duplicates:
                cleanup_plan['records_to_delete'].append({
                    'id': dup['id'],
                    'pool_name': dup['pool_name'],
                    'value': dup['value'],
                    'created_at': dup['created_at'],
                    'unique_key': unique_key
                })
                cleanup_plan['total_excess_value'] += dup['value']
            
            print(f"   üîç {newest['pool_name']} ({unique_key})")
            print(f"      ‚úÖ –û—Å—Ç–∞–≤–ª—è–µ–º: ${newest['value']:,.2f} ({newest['created_at']})")
            print(f"      ‚ùå –£–¥–∞–ª—è–µ–º: {len(duplicates)} –∑–∞–ø–∏—Å–µ–π –Ω–∞ ${sum(d['value'] for d in duplicates):,.2f}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–ª–∞–Ω –≤ JSON
    with open('cleanup_plan.json', 'w') as f:
        json.dump(cleanup_plan, f, indent=2, ensure_ascii=False)
    
    print(f"\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   ‚Ä¢ –ü–æ–∑–∏—Ü–∏–π —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏: {cleanup_plan['duplicates_found']}")
    print(f"   ‚Ä¢ –ó–∞–ø–∏—Å–µ–π –∫ —É–¥–∞–ª–µ–Ω–∏—é: {len(cleanup_plan['records_to_delete'])}")
    print(f"   ‚Ä¢ –ó–∞–ø–∏—Å–µ–π –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—é: {len(cleanup_plan['records_to_keep'])}")
    print(f"   ‚Ä¢ –ò–∑–±—ã—Ç–æ—á–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: ${cleanup_plan['total_excess_value']:,.2f}")
    
    # –°–æ–∑–¥–∞–µ–º SQL —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
    create_sql_cleanup_script(cleanup_plan['records_to_delete'])
    
    print(f"\nüíæ –§–ê–ô–õ–´ –°–û–ó–î–ê–ù–´:")
    print(f"   ‚Ä¢ cleanup_plan.json - –ø–æ–ª–Ω—ã–π –ø–ª–∞–Ω –æ—á–∏—Å—Ç–∫–∏")
    print(f"   ‚Ä¢ cleanup_duplicates_safe.sql - SQL —Å–∫—Ä–∏–ø—Ç")
    
    return cleanup_plan

def create_sql_cleanup_script(records_to_delete):
    """–°–æ–∑–¥–∞–µ—Ç SQL —Å–∫—Ä–∏–ø—Ç –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏"""
    
    if not records_to_delete:
        return
    
    script_content = """-- –ë–ï–ó–û–ü–ê–°–ù–ê–Ø –û–ß–ò–°–¢–ö–ê –î–£–ë–õ–ò–ö–ê–¢–û–í lp_position_snapshots
-- =====================================================
-- –í–ê–ñ–ù–û: –ü–µ—Ä–µ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º —Å–¥–µ–ª–∞–π—Ç–µ –±—ç–∫–∞–ø —Ç–∞–±–ª–∏—Ü—ã!
-- 
-- CREATE TABLE lp_position_snapshots_backup AS 
-- SELECT * FROM lp_position_snapshots;
-- 
"""
    script_content += f"-- –£–¥–∞–ª—è–µ—Ç {len(records_to_delete)} –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∑–∞–ø–∏—Å–µ–π\n"
    script_content += f"-- –°–æ–∑–¥–∞–Ω: {datetime.now().isoformat()}\n\n"
    
    script_content += "BEGIN;\n\n"
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø—É–ª–∞–º –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏
    pools = defaultdict(list)
    for record in records_to_delete:
        pool_name = record['pool_name']
        pools[pool_name].append(record)
    
    for pool_name, pool_records in pools.items():
        script_content += f"-- {pool_name} ({len(pool_records)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)\n"
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º ID –≤ –±–∞—Ç—á–∏ –ø–æ 20 –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        ids = [record['id'] for record in pool_records]
        batch_size = 20
        
        for i in range(0, len(ids), batch_size):
            batch = ids[i:i+batch_size]
            ids_str = "', '".join(str(id) for id in batch)
            script_content += f"DELETE FROM lp_position_snapshots WHERE id IN ('{ids_str}');\n"
        
        script_content += "\n"
    
    script_content += """
-- –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
SELECT 
    COUNT(*) as total_records,
    COUNT(DISTINCT CONCAT(position_mint, '_', network)) as unique_positions
FROM lp_position_snapshots 
WHERE created_at >= NOW() - INTERVAL '7 days';

COMMIT;

-- ROLLBACK; -- –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —ç—Ç—É —Å—Ç—Ä–æ–∫—É –≤–º–µ—Å—Ç–æ COMMIT –¥–ª—è –æ—Ç–∫–∞—Ç–∞
"""
    
    with open('cleanup_duplicates_safe.sql', 'w') as f:
        f.write(script_content)

if __name__ == "__main__":
    plan = create_safe_cleanup()
    
    print(f"\nüéØ –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
    print(f"   1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ cleanup_plan.json")
    print(f"   2. –°–¥–µ–ª–∞–π—Ç–µ –±—ç–∫–∞–ø —Ç–∞–±–ª–∏—Ü—ã –≤ Supabase")
    print(f"   3. –í—ã–ø–æ–ª–Ω–∏—Ç–µ cleanup_duplicates_safe.sql")
    print(f"   4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
    print(f"   5. –ò—Å–ø—Ä–∞–≤—å—Ç–µ –ª–æ–≥–∏–∫—É –∑–∞–ø–∏—Å–∏ –ø–æ–∑–∏—Ü–∏–π")
