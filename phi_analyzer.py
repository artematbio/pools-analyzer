import os
import json
import httpx
import asyncio
from datetime import datetime, timedelta
import re
from typing import List, Dict, Optional
from openai import OpenAI

# Constants
OPENAI_API_URL = "https://api.openai.com/v1/chat/completions"
MODEL_NAME = "gpt-4.1"  # –∏–ª–∏ "gpt-4" –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ –∏–º–µ–Ω–Ω–æ –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å

# Load environment variables
# Replace with your actual OpenAI API key or set as environment variable
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY', 'your_openai_api_key_here'))

def get_report_files_current_week() -> List[Dict[str, str]]:
    """Get text report files from Monday of current week to today (inclusive)."""
    # Look for files matching the pattern raydium_pool_report_*.txt
    all_report_files = [f for f in os.listdir() if f.startswith("raydium_pool_report_") and f.endswith(".txt")]
    
    if not all_report_files:
        return []
    
    # Parse dates from filenames and sort by date
    dated_files = []
    for filename in all_report_files:
        # Extract timestamp from filename: raydium_pool_report_YYYYMMDD_HHMMSS.txt
        match = re.search(r'raydium_pool_report_(\d{8})_(\d{6})\.txt', filename)
        if match:
            date_str = match.group(1)  # YYYYMMDD
            time_str = match.group(2)  # HHMMSS
            try:
                file_date = datetime.strptime(f"{date_str}_{time_str}", "%Y%m%d_%H%M%S")
                dated_files.append({
                    "filename": filename,
                    "date": file_date,
                    "date_str": file_date.strftime("%Y-%m-%d"),
                    "time_str": file_date.strftime("%H:%M:%S"),
                    "weekday": file_date.weekday(),  # 0=Monday, 6=Sunday
                    "weekday_name": file_date.strftime("%A")
                })
            except ValueError:
                continue
    
    # Sort by date (newest first)
    dated_files.sort(key=lambda x: x["date"], reverse=True)
    
    # Calculate start of current week (Monday)
    today = datetime.now().date()
    today_weekday = today.weekday()  # 0=Monday, 6=Sunday
    
    # Calculate how many days to go back to reach Monday
    days_since_monday = today_weekday
    monday_date = today - timedelta(days=days_since_monday)
    
    # Generate all dates from Monday to today
    target_dates = []
    current_date = monday_date
    while current_date <= today:
        target_dates.append(current_date)
        current_date += timedelta(days=1)
    
    # Find latest file for each day of the week
    result_files = []
    for target_date in target_dates:
        target_date_str = target_date.strftime("%Y-%m-%d")
        # Find the latest file for this date
        for file_info in dated_files:
            if file_info["date_str"] == target_date_str:
                result_files.append(file_info)
                break
    
    # Return in chronological order (Monday first, today last)
    result_files.reverse()
    
    return result_files

def read_report_content(filename: str) -> Optional[str]:
    """Read and return content of a report file."""
    try:
        with open(filename, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        print(f"Error reading {filename}: {e}")
        return None

def extract_detailed_pool_data(content: str, date_str: str, time_str: str) -> Dict:
    """Extract detailed pool data with better understanding of time context."""
    data = {
        "report_date": date_str,
        "report_time": time_str,
        "is_early_morning": time_str < "06:00:00",  # Report created early morning
        "pools": {}
    }
    
    # Extract total portfolio stats
    pos_match = re.search(r'–í—Å–µ–≥–æ CLMM –ø–æ–∑–∏—Ü–∏–π:\s*(\d+)', content)
    value_match = re.search(r'–û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –≤—Å–µ—Ö –ø–æ–∑–∏—Ü–∏–π:\s*\$([0-9,]+\.?\d*)', content)
    
    if pos_match:
        data["total_positions"] = int(pos_match.group(1))
    if value_match:
        data["total_value_usd"] = float(value_match.group(1).replace(',', ''))
    
    # Find all pool sections
    pool_pattern = r'--- –ê–ù–ê–õ–ò–ó –ü–£–õ–ê: ([^(]+)\s*\([^)]+\) ---(.*?)(?=--- –ê–ù–ê–õ–ò–ó –ü–£–õ–ê:|–î–†–£–ì–ò–ï –ü–£–õ–´|$)'
    pool_matches = re.findall(pool_pattern, content, re.DOTALL)
    
    for pool_name, pool_content in pool_matches:
        pool_name = pool_name.strip()
        pool_data = {"name": pool_name}
        
        # Extract basic metrics
        tvl_match = re.search(r'–û–±—â–∞—è –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å –ø—É–ª–∞ \(TVL\):\s*\$([0-9,]+\.?\d*)', pool_content)
        vol_24h_match = re.search(r'–û–±—ä–µ–º —Ç–æ—Ä–≥–æ–≤ –∑–∞ 24 —á–∞—Å–∞:\s*\$([0-9,]+\.?\d*)', pool_content)
        vol_7d_match = re.search(r'–û–±—ä–µ–º —Ç–æ—Ä–≥–æ–≤ –∑–∞ 7 –¥–Ω–µ–π:\s*\$([0-9,]+\.?\d*)', pool_content)
        
        if tvl_match:
            pool_data["tvl_usd"] = float(tvl_match.group(1).replace(',', ''))
        if vol_24h_match:
            pool_data["volume_24h_usd"] = float(vol_24h_match.group(1).replace(',', ''))
        if vol_7d_match:
            pool_data["volume_7d_usd"] = float(vol_7d_match.group(1).replace(',', ''))
        
        # Extract position data
        pos_count_match = re.search(r'–ê–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏:\s*(\d+)', pool_content)
        pos_value_match = re.search(r'–û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø–æ–∑–∏—Ü–∏–π:\s*~?\$([0-9,]+\.?\d*)', pool_content)
        pending_yield_match = re.search(r'–û–±—â–∏–π Pending Yield:\s*~?\$([0-9,]+\.?\d*)', pool_content)
        
        if pos_count_match:
            pool_data["positions_count"] = int(pos_count_match.group(1))
        if pos_value_match:
            pool_data["positions_value_usd"] = float(pos_value_match.group(1).replace(',', ''))
        if pending_yield_match:
            pool_data["pending_yield_usd"] = float(pending_yield_match.group(1).replace(',', ''))
        
        # Extract daily volumes (skip today if report is early morning)
        daily_volumes = []
        daily_volume_pattern = r'- (\d{4}-\d{2}-\d{2}):\s*\$([0-9,]+\.?\d*)'
        daily_matches = re.findall(daily_volume_pattern, pool_content)
        
        for vol_date, volume in daily_matches:
            volume_float = float(volume.replace(',', ''))
            # Skip today's volume if it's 0 and report is early morning
            if vol_date == date_str and volume_float == 0.0 and data["is_early_morning"]:
                continue
            daily_volumes.append({
                "date": vol_date,
                "volume_usd": volume_float
            })
        
        pool_data["daily_volumes"] = daily_volumes
        
        # Extract BitQuery data quality indicators
        bitquery_records = re.search(r'–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π \(–∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö\):\s*(\d+)', pool_content)
        bitquery_trades = re.search(r'–û–±—â–µ–µ –∫–æ–ª-–≤–æ —Å–¥–µ–ª–æ–∫:\s*(\d+)', pool_content)
        
        if bitquery_records:
            pool_data["bitquery_records"] = int(bitquery_records.group(1))
        if bitquery_trades:
            pool_data["bitquery_trades"] = int(bitquery_trades.group(1))
        
        # Data quality assessment
        pool_data["data_quality"] = "good"
        if pool_data.get("bitquery_records", 0) == 0:
            pool_data["data_quality"] = "no_historical_data"
        elif pool_data.get("volume_7d_usd", 0) == 0 and pool_data.get("volume_24h_usd", 0) > 0:
            pool_data["data_quality"] = "partial_data"
        
        data["pools"][pool_name] = pool_data
    
    return data

def create_smart_anomaly_prompt(reports_data: List[Dict]) -> tuple:
    """Create intelligent prompt that understands data context and quality."""
    
    system_prompt = """–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç DeFi-–∞–Ω–∞–ª–∏—Ç–∏–∫ —Å –≥–ª—É–±–æ–∫–∏–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö CLMM –ø—É–ª–æ–≤.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö:

üìä –û–°–û–ë–ï–ù–ù–û–°–¢–ò –î–ê–ù–ù–´–• BitQuery:
- –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π (–æ–±—ã—á–Ω–æ 6-12 —á–∞—Å–æ–≤)
- –î–∞–Ω–Ω—ã–µ –∑–∞ —Ç–µ–∫—É—â–∏–π –¥–µ–Ω—å –º–æ–≥—É—Ç –±—ã—Ç—å –ù–ï–ü–û–õ–ù–´–ú–ò –¥–∞–∂–µ –≤–æ –≤—Ç–æ—Ä–æ–π –ø–æ–ª–æ–≤–∏–Ω–µ –¥–Ω—è
- –î–∞–Ω–Ω—ã–µ –∑–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–Ω–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –ü–û–õ–ù–´–ï 24-—á–∞—Å–æ–≤—ã–µ –ø–µ—Ä–∏–æ–¥—ã
- –ü–æ—ç—Ç–æ–º—É —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ "—Å–µ–≥–æ–¥–Ω—è < –≤—á–µ—Ä–∞" —á–∞—Å—Ç–æ –ù–ï –æ–∑–Ω–∞—á–∞–µ—Ç —Å–Ω–∏–∂–µ–Ω–∏–µ, –∞ –ø—Ä–æ—Å—Ç–æ –Ω–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

üö´ –ù–ï –°–ß–ò–¢–ê–ô –°–ù–ò–ñ–ï–ù–ò–ï–ú:
- –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –∑–∞ —Ç–µ–∫—É—â–∏–π –¥–µ–Ω—å –º–µ–Ω—å—à–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –¥–Ω—è - —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ—Å—Ç–æ –Ω–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
- –ï—Å–ª–∏ –æ–±—ä–µ–º —Å–µ–≥–æ–¥–Ω—è = 0 –∏ –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω —Ä–∞–Ω–æ —É—Ç—Ä–æ–º - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
- –ï—Å–ª–∏ 7-–¥–Ω–µ–≤–Ω—ã–π –æ–±—ä–µ–º = 0 –Ω–æ –µ—Å—Ç—å –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - —ç—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞ API, –Ω–µ —Ä—ã–Ω–∫–∞

‚úÖ –ê–ù–ê–õ–ò–ó–ò–†–£–ô –ö–ê–ö –†–ï–ê–õ–¨–ù–´–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø:
- –°—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ–∂–¥—É –ø–æ–ª–Ω—ã–º–∏ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–Ω—è–º–∏ (–≤—á–µ—Ä–∞ vs –ø–æ–∑–∞–≤—á–µ—Ä–∞ vs 3 –¥–Ω—è –Ω–∞–∑–∞–¥)
- –ù–µ–¥–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã –ø–æ –ø–æ–ª–Ω—ã–º –¥–Ω—è–º
- –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ TVL –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø–æ–∑–∏—Ü–∏–π (—ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ)
- –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø–æ–∑–∏—Ü–∏–π –∏ yield (—ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ)

üéØ –§–û–ö–£–° –ê–ù–ê–õ–ò–ó–ê:
1. –†–µ–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ–∂–¥—É –ü–û–õ–ù–´–ú–ò –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –ø–µ—Ä–∏–æ–¥–∞–º–∏
2. –ù–µ–¥–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (–∫–∞–∫–∏–µ –¥–Ω–∏ –Ω–µ–¥–µ–ª–∏ –∞–∫—Ç–∏–≤–Ω–µ–µ)
3. –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –ø–æ–∑–∏—Ü–∏—è—Ö –∏ TVL
4. –ü—Ä–æ–±–ª–µ–º—ã —Å –∫–∞—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö BitQuery
5. –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã (—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ–¥–µ–ª—å –º–µ–∂–¥—É —Å–æ–±–æ–π)

‚ö†Ô∏è –û–°–û–ë–´–ï –£–ö–ê–ó–ê–ù–ò–Ø:
- –í—Å–µ–≥–¥–∞ —É–ø–æ–º–∏–Ω–∞–π –æ –Ω–µ–ø–æ–ª–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞ —Ç–µ–∫—É—â–∏–π –¥–µ–Ω—å
- –ï—Å–ª–∏ –≤–∏–¥–∏—à—å "—Å–Ω–∏–∂–µ–Ω–∏–µ" –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å - –ø–æ—è—Å–Ω–∏, —á—Ç–æ —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –¥–∞–Ω–Ω—ã—Ö
- –ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É–π—Å—è –Ω–∞ —Ç—Ä–µ–Ω–¥–∞—Ö –º–µ–∂–¥—É –ø–æ–ª–Ω—ã–º–∏ –¥–Ω—è–º–∏
- –ü—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –æ–±—ä–µ–º–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π —Ñ—Ä–∞–∑—ã —Ç–∏–ø–∞ "–Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–Ω–∏"

–û–¢–í–ï–ß–ê–ô –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –∏ –ø–æ –¥–µ–ª—É."""
    
    # Build intelligent data summary
    data_summary = "–î–ê–ù–ù–´–ï –ó–ê –¢–ï–ö–£–©–£–Æ –ù–ï–î–ï–õ–Æ:\n\n"
    
    # Calculate week info
    if reports_data:
        monday_date = reports_data[0].get('date_str', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        today_date = reports_data[-1].get('date_str', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        today_time = reports_data[-1].get('time_str', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        data_summary += f"üìÖ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–π –ø–µ—Ä–∏–æ–¥: {monday_date} (–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫) ‚Üí {today_date}\n"
        data_summary += f"üìä –î–Ω–µ–π –≤ –∞–Ω–∞–ª–∏–∑–µ: {len(reports_data)}\n"
        data_summary += f"‚ö†Ô∏è –í–ê–ñ–ù–û: –î–∞–Ω–Ω—ã–µ –∑–∞ {today_date} (–ø–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç—á–µ—Ç –≤ {today_time}) –º–æ–≥—É—Ç –±—ã—Ç—å –ù–ï–ü–û–õ–ù–´–ú–ò –∏–∑-–∑–∞ –∑–∞–¥–µ—Ä–∂–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è BitQuery\n\n"
    
    for i, report_info in enumerate(reports_data):
        weekday_name = report_info.get('weekday_name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        report_date = report_info.get('date_str', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        report_time = report_info.get('time_str', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        is_early = report_info.get('is_early_morning', False)
        
        data_summary += f"=== {weekday_name.upper()} ({report_date} –≤ {report_time}) ===\n"
        if is_early:
            data_summary += "‚ö†Ô∏è –û–¢–ß–ï–¢ –°–û–ó–î–ê–ù –†–ê–ù–û –£–¢–†–û–ú - –æ–±—ä–µ–º—ã –∑–∞ —Ç–µ–∫—É—â–∏–π –¥–µ–Ω—å –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ–ø–æ–ª–Ω—ã–º–∏\n"
        
        if 'metrics' in report_info:
            metrics = report_info['metrics']
            data_summary += f"–ü–æ—Ä—Ç—Ñ–µ–ª—å: {metrics.get('total_positions', '–Ω/–¥')} –ø–æ–∑–∏—Ü–∏–π, ${metrics.get('total_value_usd', 0):,.2f}\n\n"
            
            for pool_name, pool in metrics.get('pools', {}).items():
                data_summary += f"üèä {pool_name}:\n"
                data_summary += f"   TVL: ${pool.get('tvl_usd', 0):,.2f}\n"
                data_summary += f"   –û–±—ä–µ–º 24—á: ${pool.get('volume_24h_usd', 0):,.2f}\n"
                data_summary += f"   –û–±—ä–µ–º 7–¥: ${pool.get('volume_7d_usd', 0):,.2f}\n"
                data_summary += f"   –ü–æ–∑–∏—Ü–∏–∏: {pool.get('positions_count', 0)} (${pool.get('positions_value_usd', 0):,.2f})\n"
                
                # Data quality indicator
                quality = pool.get('data_quality', 'unknown')
                if quality == 'no_historical_data':
                    data_summary += f"   ‚ö†Ô∏è –ù–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö BitQuery\n"
                elif quality == 'partial_data':
                    data_summary += f"   ‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ\n"
                
                # Yield info
                if pool.get('pending_yield_usd'):
                    data_summary += f"   Yield: ${pool.get('pending_yield_usd', 0):,.2f}\n"
                
                # Daily volumes (only meaningful data)
                daily_vols = pool.get('daily_volumes', [])
                if daily_vols:
                    data_summary += f"   –î–Ω–µ–≤–Ω—ã–µ –æ–±—ä–µ–º—ã: "
                    recent_vols = daily_vols[-7:]  # Last 7 meaningful days
                    vol_str = ", ".join([f"{dv['date'][-5:]}: ${dv['volume_usd']:,.0f}" for dv in recent_vols])
                    data_summary += vol_str + "\n"
                
                data_summary += "\n"
        
        data_summary += "\n"
    
    user_prompt = f"""{data_summary}

–ó–ê–î–ê–ß–ê: –ù–∞–π–¥–∏ –†–ï–ê–õ–¨–ù–´–ï –∞–Ω–æ–º–∞–ª–∏–∏ –∏ –Ω–µ–¥–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã (–ù–ï –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –¥–∞–Ω–Ω—ã—Ö):

üóìÔ∏è **–ù–ï–î–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –û–ë–™–ï–ú–û–í**
‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –î–∞–Ω–Ω—ã–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å –º–æ–≥—É—Ç –±—ã—Ç—å –ù–ï–ü–û–õ–ù–´–ú–ò!
- –°—Ä–∞–≤–Ω–∏–≤–∞–π –æ–±—ä–µ–º—ã —Ç–æ–ª—å–∫–æ –º–µ–∂–¥—É –ü–û–õ–ù–´–ú–ò –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–Ω—è–º–∏ (–Ω–µ –≤–∫–ª—é—á–∞—è –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å)
- –ï—Å–ª–∏ –≤–∏–¥–∏—à—å "—Å–Ω–∏–∂–µ–Ω–∏–µ" –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å - —ç—Ç–æ –≤–µ—Ä–æ—è—Ç–Ω–æ –Ω–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –∞ –ù–ï —Ä–µ–∞–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ
- –ò—â–∏ —Ä–µ–∑–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è (>50%) —Ç–æ–ª—å–∫–æ –º–µ–∂–¥—É –ø–æ–ª–Ω—ã–º–∏ –¥–Ω—è–º–∏
- –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–µ–¥–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã –ø–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º –¥–Ω—è–º
- –í—ã—è–≤–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏ (–Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)

üí∞ **–ù–ï–î–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–û–ó–ò–¶–ò–ô**
‚úÖ –≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ –ê–ö–¢–£–ê–õ–¨–ù–´–ï –∏ –Ω–∞–¥–µ–∂–Ω—ã–µ:
- –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø–æ–∑–∏—Ü–∏–π –≤ —Ç–µ—á–µ–Ω–∏–µ –Ω–µ–¥–µ–ª–∏
- –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø–æ–∑–∏—Ü–∏–π  
- –î–∏–Ω–∞–º–∏–∫–∞ Pending Yield –∑–∞ –Ω–µ–¥–µ–ª—é
- –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–∑–∏—Ü–∏–π –ø–æ –¥–Ω—è–º

üèóÔ∏è **–°–¢–†–£–ö–¢–£–†–ù–´–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø**
‚úÖ –≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ –ê–ö–¢–£–ê–õ–¨–ù–´–ï –∏ –Ω–∞–¥–µ–∂–Ω—ã–µ:
- –ò–∑–º–µ–Ω–µ–Ω–∏—è TVL –ø—É–ª–æ–≤ –≤ —Ç–µ—á–µ–Ω–∏–µ –Ω–µ–¥–µ–ª–∏
- –ü–æ—è–≤–ª–µ–Ω–∏–µ/–∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π
- –ü—Ä–æ–±–ª–µ–º—ã —Å –∫–∞—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö –ø–æ –¥–Ω—è–º

‚ö†Ô∏è **–ü–†–û–ë–õ–ï–ú–´ –° –î–ê–ù–ù–´–ú–ò**
- –î–Ω–∏ —Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ BitQuery
- –ù–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞ —Ç–µ–∫—É—â–∏–π –¥–µ–Ω—å (—ç—Ç–æ –ù–û–†–ú–ê–õ–¨–ù–û)
- –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É 24—á –∏ 7–¥ –æ–±—ä–µ–º–∞–º–∏ (—Ç–æ–ª—å–∫–æ –ø–æ –ø–æ–ª–Ω—ã–º –¥–Ω—è–º)
- –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏

üìà **–ù–ï–î–ï–õ–¨–ù–´–ï –¢–†–ï–ù–î–´**
- –û–±—â–∏–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ —Ç–æ—Ä–≥–æ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (–Ω–∞ –æ—Å–Ω–æ–≤–µ –ü–û–õ–ù–´–• –¥–Ω–µ–π)
- –õ—É—á—à–∏–µ –∏ —Ö—É–¥—à–∏–µ –¥–Ω–∏ –Ω–µ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—É–ª–∞ (–∏—Å–∫–ª—é—á–∞—è –ø–æ—Å–ª–µ–¥–Ω–∏–π –Ω–µ–ø–æ–ª–Ω—ã–π –¥–µ–Ω—å)
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –Ω–µ–¥–µ–ª–∏ —Å —Å–µ—Ä–µ–¥–∏–Ω–æ–π (–ù–ï —Å –ø–æ—Å–ª–µ–¥–Ω–∏–º –Ω–µ–ø–æ–ª–Ω—ã–º –¥–Ω–µ–º)

üö´ –°–¢–†–û–ì–û –ó–ê–ü–†–ï–©–ï–ù–û:
- –°—á–∏—Ç–∞—Ç—å –Ω–∏–∑–∫–∏–µ –æ–±—ä–µ–º—ã –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–Ω—è "—Å–Ω–∏–∂–µ–Ω–∏–µ–º" –∏–ª–∏ "–ø—Ä–æ–±–ª–µ–º–æ–π"
- –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–µ–Ω–¥—ã —Å –≤–∫–ª—é—á–µ–Ω–∏–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –Ω–µ–ø–æ–ª–Ω–æ–≥–æ –¥–Ω—è
- –î–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥—ã –æ "—Ä–µ–∑–∫–æ–º –ø–∞–¥–µ–Ω–∏–∏" –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–Ω—è

‚úÖ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –£–ö–ê–ó–´–í–ê–ô:
- "–ù–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–Ω–∏..."
- "–ò—Å–∫–ª—é—á–∞—è –Ω–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å..."
- "–î–∞–Ω–Ω—ã–µ –∑–∞ [–ø–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞] –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ–ø–æ–ª–Ω—ã–º–∏ –∏–∑-–∑–∞ –∑–∞–¥–µ—Ä–∂–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ —Ç—Ä–µ–Ω–¥–∞—Ö –º–µ–∂–¥—É –ü–û–õ–ù–´–ú–ò –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–Ω—è–º–∏
- –ü—Ä–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–Ω—è –≤—Å–µ–≥–¥–∞ –æ–≥–æ–≤–∞—Ä–∏–≤–∞–π –Ω–µ–ø–æ–ª–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
- –£–∫–∞–∑—ã–≤–∞–π –ö–û–ù–ö–†–ï–¢–ù–´–ï —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ–∂–¥—É –ø–æ–ª–Ω—ã–º–∏ –¥–Ω—è–º–∏
- –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–π –Ω–∞—Ö–æ–¥–∫–∏ –ø–æ –Ω–∞–¥–µ–∂–Ω—ã–º –¥–∞–Ω–Ω—ã–º (TVL, –ø–æ–∑–∏—Ü–∏–∏, yield)
- –í—ã—è–≤–∏ –Ω–µ–¥–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""

    return system_prompt, user_prompt

async def get_analysis_from_openai(api_key: str, system_prompt: str, user_prompt: str) -> Optional[str]:
    """Send request to OpenAI API and get analysis from GPT-4."""
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "max_tokens": 2500,
        "temperature": 0.1  # Low temperature for more focused, analytical responses
    }
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                OPENAI_API_URL,
                headers=headers,
                json=payload,
                timeout=300  # 5-minute timeout
            )
            
            response.raise_for_status()
            data = response.json()
            
            # Extract the response text from the API response
            if data and "choices" in data and len(data["choices"]) > 0:
                return data["choices"][0]["message"]["content"]
            else:
                print("API response did not contain expected data: ", data)
                return None
                
    except httpx.HTTPStatusError as e:
        print(f"HTTP error occurred: {e.response.status_code} - {e.response.text}")
        return None
    except httpx.RequestError as e:
        print(f"Request error occurred: {e}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None

def save_analysis_to_file(analysis_text: str, reports_count: int) -> str:
    """Save the received analysis to a text file with timestamp."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"weekly_anomaly_analysis_{reports_count}days_{timestamp}.txt"
    
    header = f"""–ù–ï–î–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ê–ù–û–ú–ê–õ–ò–ô RAYDIUM CLMM
–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {datetime.now().strftime('%d.%m.%Y %H:%M')}
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–Ω–µ–π: {reports_count}
–ü–µ—Ä–∏–æ–¥: –° –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫–∞ —Ç–µ–∫—É—â–µ–π –Ω–µ–¥–µ–ª–∏ –¥–æ —Å–µ–≥–æ–¥–Ω—è
–ú–æ–¥–µ–ª—å: {MODEL_NAME}
–í–µ—Ä—Å–∏—è: Weekly Smart Analysis v2.2 - –£–ª—É—á—à–µ–Ω–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –Ω–µ–ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

{'='*70}

"""
    
    with open(filename, "w", encoding="utf-8") as f:
        f.write(header + analysis_text)
    
    return filename

async def main():
    try:
        print("üß† –ó–∞–ø—É—Å–∫ –ù–ï–î–ï–õ–¨–ù–û–ì–û –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –∞–Ω–æ–º–∞–ª–∏–π –¥–ª—è Raydium CLMM...")
        print("üìÖ –í–µ—Ä—Å–∏—è 2.2 - —É–ª—É—á—à–µ–Ω–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –Ω–µ–ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        
        # Get report files for current week
        report_files = get_report_files_current_week()
        
        if not report_files:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –æ—Ç—á–µ—Ç–æ–≤ –∑–∞ —Ç–µ–∫—É—â—É—é –Ω–µ–¥–µ–ª—é.")
            print("–ó–∞–ø—É—Å—Ç–∏—Ç–µ pool_analyzer.py –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–æ–≤.")
            return
        
        # Show week summary
        if report_files:
            monday_date = report_files[0]['date_str']
            today_date = report_files[-1]['date_str']
            print(f"üìÖ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–π –ø–µ—Ä–∏–æ–¥: {monday_date} (–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫) ‚Üí {today_date}")
            print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(report_files)} –¥–Ω–µ–π:")
        
        # Load and process reports with intelligent parsing
        reports_data = []
        for file_info in report_files:
            weekday_emoji = ["üìÖ", "üìä", "üìà", "üìâ", "üìã", "üéØ", "üéâ"][file_info['weekday']]
            print(f"  {weekday_emoji} {file_info['weekday_name']}: {file_info['filename']}")
            print(f"      –í—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è: {file_info['date_str']} –≤ {file_info['time_str']}")
            
            content = read_report_content(file_info['filename'])
            if content:
                # Extract detailed metrics with context awareness
                metrics = extract_detailed_pool_data(
                    content, 
                    file_info['date_str'], 
                    file_info['time_str']
                )
                
                reports_data.append({
                    "filename": file_info['filename'],
                    "date_str": file_info['date_str'],
                    "time_str": file_info['time_str'],
                    "weekday_name": file_info['weekday_name'],
                    "is_early_morning": metrics["is_early_morning"],
                    "content": content,
                    "metrics": metrics
                })
                
                # Show data quality summary
                pools_with_issues = sum(1 for pool in metrics["pools"].values() 
                                      if pool.get("data_quality") != "good")
                if pools_with_issues:
                    print(f"      ‚ö†Ô∏è {pools_with_issues} –ø—É–ª–æ–≤ —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö")
                    
            else:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞")
        
        if not reports_data:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –∏–∑ –æ–¥–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞.")
            return
        
        print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(reports_data)} –æ—Ç—á–µ—Ç–æ–≤ –∑–∞ –Ω–µ–¥–µ–ª—é")
        
        # Create intelligent prompt for analysis
        system_prompt, user_prompt = create_smart_anomaly_prompt(reports_data)
        print("üß† –°–æ–∑–¥–∞–Ω –Ω–µ–¥–µ–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º...")
        
        # Get analysis from OpenAI API
        print("üöÄ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ OpenAI API...")
        print("‚è±Ô∏è  –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –Ω–µ–¥–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑...")
        
        analysis = await get_analysis_from_openai(os.getenv('OPENAI_API_KEY', 'your_openai_api_key_here'), system_prompt, user_prompt)
        
        if analysis:
            # Save analysis to file
            output_file = save_analysis_to_file(analysis, len(reports_data))
            print(f"\n‚úÖ –ù–µ–¥–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")
            
            # Print summary to console
            print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢ –ù–ï–î–ï–õ–¨–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê:")
            print("=" * 60)
            # Show first meaningful lines of analysis
            lines = analysis.split('\n')
            shown_lines = 0
            for line in lines:
                if line.strip():
                    print(f"  {line}")
                    shown_lines += 1
                    if shown_lines >= 15:  # Show more lines for better overview
                        break
            
            if len([l for l in lines if l.strip()]) > 15:
                print("  ...")
                print(f"  üìÑ –ü–æ–ª–Ω—ã–π –Ω–µ–¥–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤ —Ñ–∞–π–ª–µ: {output_file}")
                
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –æ—Ç OpenAI API.")
    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ main: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main()) 